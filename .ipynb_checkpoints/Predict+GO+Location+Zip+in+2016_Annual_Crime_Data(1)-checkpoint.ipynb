{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting GO Location Zip in 2016_Annual_Crime_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook automatically generated from your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Random forest (GO Zip classifier), trained on 2017-12-07 16:18:04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generated on 2017-12-07 22:31:40.568818"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction\n",
    "This notebook will reproduce the steps for a MULTICLASS on  2016_Annual_Crime_Data.\n",
    "The main objective is to predict the variable GO Location Zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with importing the required libs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import dataiku.core.pandasutils as pdu\n",
    "from dataiku.doctor.preprocessing import PCA\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And tune pandas display options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 3000)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing base data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to get our machine learning dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the preparation that you defined. You should not modify this.\n",
    "preparation_steps = []\n",
    "preparation_output_schema = {u'userModified': False, u'columns': [{u'timestampNoTzAsDate': False, u'type': u'bigint', u'name': u'GO Primary Key', u'maxLength': -1}, {u'timestampNoTzAsDate': False, u'type': u'bigint', u'name': u'Council District', u'maxLength': -1}, {u'timestampNoTzAsDate': False, u'type': u'string', u'name': u'GO Highest Offense Desc', u'maxLength': -1}, {u'timestampNoTzAsDate': False, u'type': u'string', u'name': u'Highest NIBRS/UCR Offense Description', u'maxLength': -1}, {u'timestampNoTzAsDate': False, u'type': u'string', u'name': u'GO Report Date', u'maxLength': -1}, {u'timestampNoTzAsDate': False, u'type': u'string', u'name': u'GO Location', u'maxLength': -1}, {u'timestampNoTzAsDate': False, u'downcastedToStringFromMeaning': u'Boolean', u'type': u'string', u'name': u'Clearance Status', u'maxLength': -1}, {u'timestampNoTzAsDate': False, u'type': u'string', u'name': u'Clearance Date', u'maxLength': -1}, {u'timestampNoTzAsDate': False, u'type': u'string', u'name': u'GO District', u'maxLength': -1}, {u'timestampNoTzAsDate': False, u'type': u'bigint', u'name': u'GO Location Zip', u'maxLength': -1}, {u'timestampNoTzAsDate': False, u'type': u'double', u'name': u'GO Census Tract', u'maxLength': -1}, {u'timestampNoTzAsDate': False, u'type': u'bigint', u'name': u'GO X Coordinate', u'maxLength': -1}, {u'timestampNoTzAsDate': False, u'type': u'bigint', u'name': u'GO Y Coordinate', u'maxLength': -1}]}\n",
    "\n",
    "ml_dataset_handle = dataiku.Dataset('2016_Annual_Crime_Data')\n",
    "ml_dataset_handle.set_preparation_steps(preparation_steps, preparation_output_schema)\n",
    "%time ml_dataset = ml_dataset_handle.get_dataframe(limit = 100000)\n",
    "\n",
    "print 'Base data has %i rows and %i columns' % (ml_dataset.shape[0], ml_dataset.shape[1])\n",
    "# Five first records\",\n",
    "ml_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing aims at making the dataset compatible with modeling.\n",
    "At the end of this step, we will have a matrix of float numbers, with no missing values.\n",
    "We'll use the features and the preprocessing steps defined in Models.\n",
    "\n",
    "Let's only keep selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dataset = ml_dataset[[u'GO Y Coordinate', u'GO Location Zip', u'GO Report Date', u'GO X Coordinate', u'Clearance Status', u'GO Census Tract', u'Clearance Date', u'GO Primary Key', u'Highest NIBRS/UCR Offense Description', u'GO District', u'Council District', u'GO Highest Offense Desc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first coerce categorical columns into unicode, numerical features into floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# astype('unicode') does not work as expected\n",
    "def coerce_to_unicode(x):\n",
    "    if isinstance(x, str):\n",
    "        return unicode(x,'utf-8')\n",
    "    else:\n",
    "        return unicode(x)\n",
    "\n",
    "categorical_features = [u'GO Report Date', u'Clearance Status', u'Clearance Date', u'Highest NIBRS/UCR Offense Description', u'GO District', u'GO Highest Offense Desc']\n",
    "numerical_features = [u'GO Y Coordinate', u'GO X Coordinate', u'GO Census Tract', u'GO Primary Key', u'Council District']\n",
    "text_features = []\n",
    "from dataiku.doctor.utils import datetime_to_epoch\n",
    "for feature in categorical_features:\n",
    "    ml_dataset[feature] = ml_dataset[feature].apply(coerce_to_unicode)\n",
    "for feature in text_features:\n",
    "    ml_dataset[feature] = ml_dataset[feature].apply(coerce_to_unicode)\n",
    "for feature in numerical_features:\n",
    "    if ml_dataset[feature].dtype == np.dtype('M8[ns]'):\n",
    "        ml_dataset[feature] = datetime_to_epoch(ml_dataset[feature])\n",
    "    else:\n",
    "        ml_dataset[feature] = ml_dataset[feature].astype('double')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to handle the target variable and store it in a new variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {u'78723': 7, u'78704': 3, u'78705': 12, u'78701': 5, u'78726': 34, u'78703': 16, u'78748': 8, u'78741': 1, u'78729': 19, u'78742': 39, u'78745': 4, u'78744': 6, u'78747': 31, u'78746': 17, u'78727': 18, u'78722': 30, u'78730': 40, u'78725': 38, u'78702': 9, u'78653': 37, u'78652': 41, u'78735': 25, u'78750': 24, u'78660': 35, u'78613': 27, u'78717': 29, u'78758': 2, u'78759': 10, u'78617': 28, u'78739': 33, u'78752': 11, u'78753': 0, u'78749': 15, u'78751': 14, u'78756': 26, u'78757': 13, u'78754': 23, u'78731': 22, u'78719': 32, u'78721': 21, u'78736': 36, u'78712': 42, u'78728': 43, u'78724': 20}\n",
    "ml_dataset['__target__'] = ml_dataset['GO Location Zip'].map(str).map(target_map)\n",
    "del ml_dataset['GO Location Zip']\n",
    "\n",
    "\n",
    "# Remove rows for which the target is unknown.\n",
    "ml_dataset = ml_dataset[~ml_dataset['__target__'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset needs to be split into 2 new sets, one that will be used for training the model (train set)\n",
    "and another that will be used to test its generalization capability (test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple cross-validation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = pdu.split_train_valid(ml_dataset, prop=0.8)\n",
    "print 'Train data has %i rows and %i columns' % (train.shape[0], train.shape[1])\n",
    "print 'Test data has %i rows and %i columns' % (test.shape[0], test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do at the features level is to handle the missing values.\n",
    "Let's reuse the settings defined in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows_when_missing = []\n",
    "impute_when_missing = [{'impute_with': u'MEAN', 'feature': u'GO Y Coordinate'}, {'impute_with': u'MEAN', 'feature': u'GO X Coordinate'}, {'impute_with': u'MEAN', 'feature': u'GO Census Tract'}, {'impute_with': u'MEAN', 'feature': u'GO Primary Key'}, {'impute_with': u'MEAN', 'feature': u'Council District'}]\n",
    "\n",
    "# Features for which we drop rows with missing values\"\n",
    "for feature in drop_rows_when_missing:\n",
    "    train = train[train[feature].notnull()]\n",
    "    test = test[test[feature].notnull()]\n",
    "    print 'Dropped missing records in %s' % feature\n",
    "\n",
    "# Features for which we impute missing values\"\n",
    "for feature in impute_when_missing:\n",
    "    if feature['impute_with'] == 'MEAN':\n",
    "        v = train[feature['feature']].mean()\n",
    "    elif feature['impute_with'] == 'MEDIAN':\n",
    "        v = train[feature['feature']].median()\n",
    "    elif feature['impute_with'] == 'CREATE_CATEGORY':\n",
    "        v = 'NULL_CATEGORY'\n",
    "    elif feature['impute_with'] == 'MODE':\n",
    "        v = train[feature['feature']].value_counts().index[0]\n",
    "    elif feature['impute_with'] == 'CONSTANT':\n",
    "        v = feature['value']\n",
    "    train[feature['feature']] = train[feature['feature']].fillna(v)\n",
    "    test[feature['feature']] = test[feature['feature']].fillna(v)\n",
    "    print 'Imputed missing values in feature %s with value %s' % (feature['feature'], unicode(str(v), 'utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now handle the categorical features (still using the settings defined in Models):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dummy-encode the following features.\n",
    "A binary column is created for each of the 100 most frequent values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_DUMMIES = 100\n",
    "\n",
    "categorical_to_dummy_encode = [u'GO Report Date', u'Clearance Status', u'Clearance Date', u'Highest NIBRS/UCR Offense Description', u'GO District', u'GO Highest Offense Desc']\n",
    "\n",
    "# Only keep the top 100 values\n",
    "def select_dummy_values(train, features):\n",
    "    dummy_values = {}\n",
    "    for feature in categorical_to_dummy_encode:\n",
    "        values = [\n",
    "            value\n",
    "            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n",
    "        ]\n",
    "        dummy_values[feature] = values\n",
    "    return dummy_values\n",
    "\n",
    "DUMMY_VALUES = select_dummy_values(train, categorical_to_dummy_encode)\n",
    "\n",
    "def dummy_encode_dataframe(df):\n",
    "    for (feature, dummy_values) in DUMMY_VALUES.items():\n",
    "        for dummy_value in dummy_values:\n",
    "            dummy_name = u'%s_value_%s' % (feature, unicode(dummy_value))\n",
    "            df[dummy_name] = (df[feature] == dummy_value).astype(float)\n",
    "        del df[feature]\n",
    "        print 'Dummy-encoded feature %s' % feature\n",
    "\n",
    "dummy_encode_dataframe(train)\n",
    "\n",
    "dummy_encode_dataframe(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rescale numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_features = {u'GO Y Coordinate': u'AVGSTD', u'Council District': u'AVGSTD', u'GO X Coordinate': u'AVGSTD', u'GO Census Tract': u'AVGSTD', u'GO Primary Key': u'AVGSTD'}\n",
    "for (feature_name, rescale_method) in rescale_features.items():\n",
    "    if rescale_method == 'MINMAX':\n",
    "        _min = train[feature_name].min()\n",
    "        _max = train[feature_name].max()\n",
    "        scale = _max - _min\n",
    "        shift = _min\n",
    "    else:\n",
    "        shift = train[feature_name].mean()\n",
    "        scale = train[feature_name].std()\n",
    "    if scale == 0.:\n",
    "        del train[feature_name]\n",
    "        del test[feature_name]\n",
    "        print 'Feature %s was dropped because it has no variance' % feature_name\n",
    "    else:\n",
    "        print 'Rescaled %s' % feature_name\n",
    "        train[feature_name] = (train[feature_name] - shift).astype(np.float64) / scale\n",
    "        test[feature_name] = (test[feature_name] - shift).astype(np.float64) / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before actually creating our model, we need to split the datasets into their features and labels parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.drop('__target__', axis=1)\n",
    "test_X = test.drop('__target__', axis=1)\n",
    "\n",
    "train_Y = np.array(train['__target__'])\n",
    "test_Y = np.array(test['__target__'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally create our model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "    n_jobs=4,\n",
    "    random_state=1337,\n",
    "    max_depth=13,\n",
    "    min_samples_leaf=1,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... And train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build up our result dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now being trained, we can apply it to our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _predictions = clf.predict(test_X)\n",
    "%time _probas = clf.predict_proba(test_X)\n",
    "predictions = pd.Series(data=_predictions, index=test_X.index, name='predicted_value')\n",
    "cols = [\n",
    "    u'probability_of_value_%s' % label\n",
    "    for (_, label) in sorted([(int(target_map[label]), label) for label in target_map])\n",
    "]\n",
    "probabilities = pd.DataFrame(data=_probas, index=test_X.index, columns=cols)\n",
    "\n",
    "# Build scored dataset\n",
    "results_test = test_X.join(predictions, how='left')\n",
    "results_test = results_test.join(probabilities, how='left')\n",
    "results_test = results_test.join(test['__target__'], how='left')\n",
    "results_test = results_test.rename(columns= {'__target__': 'GO Location Zip'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_data = []\n",
    "features = train_X.columns\n",
    "for feature_name, feature_importance in zip(features, clf.feature_importances_):\n",
    "    feature_importances_data.append({\n",
    "        'feature': feature_name,\n",
    "        'importance': feature_importance\n",
    "    })\n",
    "\n",
    "# Plot the results\n",
    "pd.DataFrame(feature_importances_data)\\\n",
    "    .set_index('feature')\\\n",
    "    .sort_values(by='importance')[-10::]\\\n",
    "    .plot(title='Top 10 most important variables',\n",
    "          kind='barh',\n",
    "          figsize=(10, 6),\n",
    "          color='#348ABD',\n",
    "          alpha=0.6,\n",
    "          lw='1',\n",
    "          edgecolor='#348ABD',\n",
    "          grid=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can measure the model's accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataiku.doctor.utils.metrics import mroc_auc_score\n",
    "test_Y_ser = pd.Series(test_Y)\n",
    "print 'AUC value:', mroc_auc_score(test_Y_ser, _probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the predictions directly.\n",
    "Since scikit-learn only predicts numericals, the labels have been mapped to 0,1,2 ...\n",
    "We need to 'reverse' the mapping to display the initial labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = { target_map[label] : label for label in target_map}\n",
    "predictions.map(inv_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. It's now up to you to tune your preprocessing, your algo, and your analysis !\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "Predicting GO Location Zip in 2016_Annual_Crime_Data"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
